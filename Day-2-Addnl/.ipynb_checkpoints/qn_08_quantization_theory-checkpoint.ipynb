{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6tCv-A_bJVb"
   },
   "source": [
    "#### Quantization Theory\n",
    "\n",
    "-  perform Linear Quantization.\n",
    "\n",
    "```Python\n",
    "!pip install transformers==4.35.0\n",
    "!pip install quanto==0.0.11\n",
    "!pip install torch==2.1.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install quanto==0.0.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5-FLAN\n",
    "\n",
    "\n",
    "For the T5-FLAN model, here is one more library to install if you are running locally:\n",
    "```Python\n",
    "!pip install sentencepiece==0.2.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentencepiece==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from helper import compute_module_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", cache_dir=r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> annie scott</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, my name is \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 0.307844608 GB\n"
     ]
    }
   ],
   "source": [
    "from helper import compute_module_sizes\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize the model (8-bit precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from quanto import quantize, freeze\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "quantize(model, weights=torch.int8, activations=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze the model\n",
    "\n",
    "- This will work fine with the smaller T5-Flan model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 0.12682868 GB\n"
     ]
    }
   ],
   "source": [
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try running inference on the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> annie scott</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, my name is \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqmyZYGbbO8A"
   },
   "source": [
    "\n",
    "### Without Quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load [EleutherAI/pythia-410m](https://huggingface.co/EleutherAI/pythia-410m) model and tokenizer.\n",
    "\n",
    "```Python\n",
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"EleutherAI/pythia-410m\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             low_cpu_mem_usage=True)\n",
    "print(model.gpt_neox)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a start of a (`text`) sentence which you'd like the model to complete.\n",
    "```Python\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "outputs\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the model's size using the helper function, `compute_module_sizes`.\n",
    "```Python\n",
    "from helper import compute_module_sizes\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "```\n",
    "**Note:** The weights are in `fp32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BC809CYugOp"
   },
   "source": [
    "### 8-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute '_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mquanto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize, freeze\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m quantize(model, weights\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint8, activations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# after performing quantization\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mgpt_neox)\n",
      "File \u001b[1;32mD:\\ANACONDA3\\Lib\\site-packages\\quanto\\quantize.py:27\u001b[0m, in \u001b[0;36mquantize\u001b[1;34m(model, modules, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m qmodule \u001b[38;5;241m=\u001b[39m quantize_module(m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qmodule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     set_module_by_name(model, name, qmodule)\n",
      "File \u001b[1;32mD:\\ANACONDA3\\Lib\\site-packages\\quanto\\nn\\qmodule.py:52\u001b[0m, in \u001b[0;36mquantize_module\u001b[1;34m(module, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     51\u001b[0m                 module_kwargs[name] \u001b[38;5;241m=\u001b[39m kwargs[name]\n\u001b[1;32m---> 52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m qcls\u001b[38;5;241m.\u001b[39mfrom_module(module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ANACONDA3\\Lib\\site-packages\\quanto\\nn\\qlinear.py:29\u001b[0m, in \u001b[0;36mQLinear.from_module\u001b[1;34m(cls, module, weights, activations)\u001b[0m\n\u001b[0;32m     20\u001b[0m qmodule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     21\u001b[0m     module\u001b[38;5;241m.\u001b[39min_features,\n\u001b[0;32m     22\u001b[0m     module\u001b[38;5;241m.\u001b[39mout_features,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     activations\u001b[38;5;241m=\u001b[39mactivations,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 29\u001b[0m     qmodule\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcopy_(module\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m         qmodule\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mcopy_(module\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mD:\\ANACONDA3\\Lib\\site-packages\\quanto\\tensor\\core.py:291\u001b[0m, in \u001b[0;36mQTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Defer to dispatcher to look instead for QTensor operations\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ANACONDA3\\Lib\\site-packages\\quanto\\tensor\\core.py:309\u001b[0m, in \u001b[0;36mQTensor.__torch_dispatch__\u001b[1;34m(cls, op, types, args, kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, QTensor) \u001b[38;5;129;01mor\u001b[39;00m arg\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m qarg\u001b[38;5;241m.\u001b[39maxis:\n\u001b[0;32m    307\u001b[0m                 \u001b[38;5;66;03m# Incompatible argument detected: qfallback\u001b[39;00m\n\u001b[0;32m    308\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m qfallback(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qdispatch\u001b[38;5;241m.\u001b[39mqop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# No dispatch available: qfallback\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qfallback(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ANACONDA3\\Lib\\site-packages\\quanto\\tensor\\ops.py:139\u001b[0m, in \u001b[0;36mcopy_\u001b[1;34m(op, dest, src)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;129m@register_qtensor_op\u001b[39m([torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mcopy_])\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy_\u001b[39m(op, dest, src):\n\u001b[1;32m--> 139\u001b[0m     dest\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m op(dest\u001b[38;5;241m.\u001b[39m_data, src\u001b[38;5;241m.\u001b[39m_data)\n\u001b[0;32m    140\u001b[0m     dest\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;241m=\u001b[39m op(dest\u001b[38;5;241m.\u001b[39m_scale, src\u001b[38;5;241m.\u001b[39m_scale)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dest\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Parameter' object has no attribute '_data'"
     ]
    }
   ],
   "source": [
    "from quanto import quantize, freeze\n",
    "import torch\n",
    "\n",
    "quantize(model, weights=torch.int8, activations=None)\n",
    "# after performing quantization\n",
    "print(model.gpt_neox)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The \"freeze\" function requires more memory than is available in this classroom.\n",
    "- This code will run on a machine that has 8GB of memory, and so it will likely work if you run this code on your local machine.\n",
    "\n",
    "```Python\n",
    "# freeze the model\n",
    "freeze(model)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "\n",
    "# get model size after quantization\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "\n",
    "# run inference after quantizing the model\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing \"linear quantization\" to \"downcasting\"\n",
    "\n",
    "To recap the difference between the \"linear quantization\" method in this lesson with the \"downcasting\" method in the previous lesson:\n",
    "\n",
    "- When downcasting a model, you convert the model's parameters to a more compact data type (bfloat16).\n",
    "\n",
    "- During inference, the model performs its calculations in this data type, and its activations are in this data type.\n",
    "\n",
    "- Downcasting may work with the bfloat16 data type, but the model performance will likely degrade with any smaller data type, and won't work if you convert to an integer data type (like the int8 in this lesson).\n",
    "\n",
    "\n",
    "- we used another quantization method, \"linear quantization\", which enables the quantized model to maintain performance much closer to the original model by converting from the compressed data type back to the original FP32 data type during inference.\n",
    "\n",
    "- So when the model makes a prediction, it is performing the matrix multiplications in FP32, and the activations are in FP32.\n",
    "\n",
    "- This enables you to quantize the model in data types smaller than bfloat16, such as int8, in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpTGuZzDRr5A8ocSoYIFkP",
   "collapsed_sections": [
    "4V_amrl-xG9D",
    "dODA6rR0z297"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
