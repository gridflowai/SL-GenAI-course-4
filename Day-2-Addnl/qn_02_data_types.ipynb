{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5be150a-d62b-4b18-a856-821e11391301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2c99e-9665-4de2-bc22-c9b5dbc60640",
   "metadata": {},
   "source": [
    "#### Unsigned Integer in `n` Bits\n",
    "\n",
    "- **Definition**: An unsigned integer in `n` bits is a non-negative integer that can be represented using `n` binary digits (bits). Each bit can be either 0 or 1.\n",
    "\n",
    "- **Range**: The range of values that can be represented by an `n`-bit unsigned integer is from 0 to \\($2^n - 1$\\). \n",
    "\n",
    "- **Example**:\n",
    "  - For `n = 3`, the possible values are:\n",
    "    - Binary: `000`, `001`, `010`, `011`, `100`, `101`, `110`, `111`\n",
    "    - Decimal: 0, 1, 2, 3, 4, 5, 6, 7\n",
    "\n",
    "- **Maximum Value**: The maximum value of an `n`-bit unsigned integer is \\($2^n - 1$\\). For instance:\n",
    "  - For `n = 4`, the maximum value is \\($2^4 - 1 = 15$\\).\n",
    "\n",
    "- **Binary Representation**: The binary representation of an `n`-bit unsigned integer always has exactly `n` bits, with leading zeros added if necessary to fill the bit width.\n",
    "\n",
    "- **Usage**: Unsigned integers are commonly used in computer systems for operations where only non-negative values are needed, such as indexing, counters, and bit manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f17ce7-ec36-46c8-8245-8777d94a825a",
   "metadata": {},
   "source": [
    "#### 8-Bit Unsigned Integer\n",
    "\n",
    "- **Definition**: An 8-bit unsigned integer is a non-negative integer represented using 8 binary digits (bits). Each bit can be either 0 or 1.\n",
    "\n",
    "- **Range**: The range of values for an 8-bit unsigned integer is from 0 to \\(2^8 - 1\\), which is 0 to 255.\n",
    "\n",
    "- **Example**:\n",
    "  - Binary: `00000000` to `11111111`\n",
    "  - Decimal: 0 to 255\n",
    "\n",
    "- **Maximum Value**: The maximum value of an 8-bit unsigned integer is \\($2^8 - 1 = 255$\\).\n",
    "\n",
    "- **Binary Representation**: The binary representation is always 8 bits long, with leading zeros added if necessary to fill the bit width.\n",
    "\n",
    "- **Usage**: 8-bit unsigned integers are used for data storage and operations where only non-negative values in the range 0-255 are required, such as in color values in images and small counters.\n",
    "\n",
    "#### 8-Bit Signed Integer\n",
    "\n",
    "- **Definition**: An 8-bit signed integer can represent both positive and negative integers using 8 bits, where one bit is reserved for the sign (0 for positive, 1 for negative).\n",
    "\n",
    "- **Range**: The range of values for an 8-bit signed integer is from \\($-2^7$\\) to \\($2^7$ - 1\\), which is -128 to 127.\n",
    "\n",
    "- **Example**:\n",
    "  - Binary: \n",
    "    - Positive values: `00000000` to `01111111` (0 to 127)\n",
    "    - Negative values: `10000000` to `11111111` (-128 to -1, using two's complement representation)\n",
    "  - Decimal: -128 to 127\n",
    "\n",
    "- **Maximum and Minimum Values**:\n",
    "  - Maximum Value: \\($2^7 - 1 = 127$\\)\n",
    "  - Minimum Value: \\($-2^7 = -128$\\)\n",
    "\n",
    "- **Binary Representation**: The binary representation includes a sign bit. Positive values are represented directly in binary, while negative values use two's complement notation.\n",
    "\n",
    "- **Usage**: 8-bit signed integers are used for operations involving both positive and negative values in a range suitable for small integers, such as in signed arithmetic operations and certain types of signal processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625ba8a2-fcbe-4f73-b673-a2e5e8197507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=0, max=255, dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information of `8-bit unsigned integer`\n",
    "torch.iinfo(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01037ed-e5c0-456e-8ae0-d65a1347dc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-128, max=127, dtype=int8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information of `8-bit (signed) integer`\n",
    "torch.iinfo(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d02096e-cc49-405f-ad7c-5593d204c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-32768, max=32767, dtype=int16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Information of `16-bit (signed) integer`\n",
    "torch.iinfo(torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2e61e9-baea-4ea1-9284-b731004055eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-2.14748e+09, max=2.14748e+09, dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Information of `32-bit (signed) integer`\n",
    "torch.iinfo(torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a322e-7c4e-4da8-9f21-cff81ccb9064",
   "metadata": {},
   "source": [
    "#### Components of Floating-Point Numbers\n",
    "\n",
    "- **Sign**:\n",
    "  - **Definition**: Indicates whether the number is positive or negative.\n",
    "  - **Bit Representation**: Typically represented by a single bit in the floating-point format (0 for positive, 1 for negative).\n",
    "\n",
    "- **Exponent**:\n",
    "  - **Definition**: Determines the magnitude of the number by specifying the power of the base (usually base 2 in binary systems).\n",
    "  - **Bit Representation**: The exponent is stored in a biased form. For example, in IEEE 754 single-precision format, the exponent is an 8-bit field with a bias of 127.\n",
    "\n",
    "- **Mantissa (Significand)**:\n",
    "  - **Definition**: Represents the significant digits of the number. It is the precision part of the floating-point number.\n",
    "  - **Bit Representation**: The mantissa is stored as a binary fraction. In IEEE 754 single-precision format, the mantissa is an 23-bit field. The leading bit (implicit 1 in normalized numbers) is not stored but assumed.\n",
    "\n",
    "- **Normalization** (for normalized numbers):\n",
    "  - **Definition**: The process of adjusting the mantissa and exponent to ensure that the leading digit of the mantissa is non-zero (usually 1 for binary systems).\n",
    "  - **Binary Representation**: For normalized numbers, the mantissa always starts with a leading 1 which is implicit and not explicitly stored.\n",
    "\n",
    "- **Example** (IEEE 754 Single-Precision):\n",
    "  - **Binary Representation**: `0 10000001 01100000000000000000000`\n",
    "    - **Sign Bit**: `0` (positive)\n",
    "    - **Exponent**: `10000001` (129 in decimal, with a bias of 127, so the actual exponent is \\(129 - 127 = 2\\))\n",
    "    - **Mantissa**: `01100000000000000000000` (1.011 in binary)\n",
    "\n",
    "- **Formula**:\n",
    "  - The floating-point number is calculated as:\n",
    "    \\[\n",
    "    \\text{Value} = (-1)^{\\text{sign}} \\times (1 + \\text{mantissa}) \\times 2^{\\text{exponent} - \\text{bias}}\n",
    "    \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b23dc6-b140-44cb-ab2f-0972f89430ee",
   "metadata": {},
   "source": [
    "#### FP32 (32-bit Floating-Point) Format\n",
    "\n",
    "- **Definition**: FP32, also known as single-precision floating-point format, is a standard format for representing floating-point numbers using 32 bits. It is defined by the IEEE 754 standard.\n",
    "\n",
    "- **Components**:\n",
    "  - **Sign Bit**:\n",
    "    - **Bit Position**: 1 bit\n",
    "    - **Definition**: Indicates whether the number is positive or negative. `0` for positive, `1` for negative.\n",
    "\n",
    "  - **Exponent**:\n",
    "    - **Bit Position**: 8 bits\n",
    "    - **Definition**: Determines the scale or magnitude of the number. The exponent is stored with a bias of 127.\n",
    "    - **Range**: The exponent field allows values from 0 to 255. However, values 0 and 255 are reserved for special cases like denormalized numbers and infinity.\n",
    "\n",
    "  - **Mantissa (Significand)**:\n",
    "    - **Bit Position**: 23 bits\n",
    "    - **Definition**: Represents the precision part of the number. The mantissa includes an implicit leading 1 for normalized numbers, which is not explicitly stored.\n",
    "\n",
    "- **Format**:\n",
    "  - **Bit Layout**: `S EEEEEEEE MMMMMMMMMMMMMMMMMMMMMMM`\n",
    "    - **S**: Sign bit (1 bit)\n",
    "    - **E**: Exponent (8 bits)\n",
    "    - **M**: Mantissa (23 bits)\n",
    "\n",
    "- **Normalization**:\n",
    "  - For normalized numbers, the mantissa always starts with a leading 1 (implicit), followed by the fractional part.\n",
    "\n",
    "- **Special Values**:\n",
    "  - **Zero**: Represented with all bits of exponent and mantissa as `0`.\n",
    "  - **Infinity**: Represented with all bits of the exponent set to `1` and all bits of the mantissa set to `0`.\n",
    "  - **NaN (Not-a-Number)**: Represented with all bits of the exponent set to `1` and a non-zero mantissa.\n",
    "\n",
    "- **Example**:\n",
    "  - **Binary Representation**: `0 10000001 01100000000000000000000`\n",
    "    - **Sign Bit**: `0` (positive)\n",
    "    - **Exponent**: `10000001` (129 in decimal, with a bias of 127, so the actual exponent is \\(129 - 127 = 2\\))\n",
    "    - **Mantissa**: `01100000000000000000000` (1.011 in binary)\n",
    "  - **Decimal Value**: \\((1 + 0.011) \\times 2^2 = 1.011 \\times 4 = 4.044\\)\n",
    "\n",
    "- **Formula**:\n",
    "  - The value represented by an FP32 number is calculated as:\n",
    "    $[\n",
    "    \\text{Value} = (-1)^{\\text{sign}} \\times (1 + \\text{mantissa}) \\times 2^{\\text{exponent} - 127}\n",
    "    ]$\n",
    "\n",
    "- **Usage**:\n",
    "  - FP32 is widely used in computing, graphics, and machine learning applications for its balance between precision and memory/storage efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c25950-eaf2-4e0e-ab4c-c6229c13edb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62cb9e92-c750-4f68-bac1-ad4ad30d6bf6",
   "metadata": {},
   "source": [
    "#### FP16 (16-bit Floating-Point) Format\n",
    "\n",
    "- **Definition**: FP16, also known as half-precision floating-point format, is a compact floating-point representation using 16 bits. It is defined by the IEEE 754-2008 standard.\n",
    "\n",
    "- **Components**:\n",
    "  - **Sign Bit**:\n",
    "    - **Bit Position**: 1 bit\n",
    "    - **Definition**: Indicates whether the number is positive or negative. `0` for positive, `1` for negative.\n",
    "\n",
    "  - **Exponent**:\n",
    "    - **Bit Position**: 5 bits\n",
    "    - **Definition**: Determines the scale or magnitude of the number. The exponent is stored with a bias of 15.\n",
    "    - **Range**: The exponent field allows values from 0 to 31. However, values 0 and 31 are reserved for special cases like denormalized numbers and infinity.\n",
    "\n",
    "  - **Mantissa (Significand)**:\n",
    "    - **Bit Position**: 10 bits\n",
    "    - **Definition**: Represents the precision part of the number. The mantissa includes an implicit leading 1 for normalized numbers, which is not explicitly stored.\n",
    "\n",
    "- **Format**:\n",
    "  - **Bit Layout**: `S EEEEE MMMMMMMMMM`\n",
    "    - **S**: Sign bit (1 bit)\n",
    "    - **E**: Exponent (5 bits)\n",
    "    - **M**: Mantissa (10 bits)\n",
    "\n",
    "- **Normalization**:\n",
    "  - For normalized numbers, the mantissa always starts with a leading 1 (implicit), followed by the fractional part.\n",
    "\n",
    "- **Special Values**:\n",
    "  - **Zero**: Represented with all bits of exponent and mantissa as `0`.\n",
    "  - **Infinity**: Represented with all bits of the exponent set to `1` and all bits of the mantissa set to `0`.\n",
    "  - **NaN (Not-a-Number)**: Represented with all bits of the exponent set to `1` and a non-zero mantissa.\n",
    "\n",
    "- **Example**:\n",
    "  - **Binary Representation**: `0 10001 1010000000`\n",
    "    - **Sign Bit**: `0` (positive)\n",
    "    - **Exponent**: `10001` (17 in decimal, with a bias of 15, so the actual exponent is \\(17 - 15 = 2\\))\n",
    "    - **Mantissa**: `1010000000` (1.101 in binary)\n",
    "  - **Decimal Value**: \\((1 + 0.101) \\times 2^2 = 1.101 \\times 4 = 4.404\\)\n",
    "\n",
    "- **Formula**:\n",
    "  - The value represented by an FP16 number is calculated as:\n",
    "    \\[\n",
    "    \\text{Value} = (-1)^{\\text{sign}} \\times (1 + \\text{mantissa}) \\times 2^{\\text{exponent} - 15}\n",
    "    \\]\n",
    "\n",
    "- **Usage**:\n",
    "  - FP16 is used in various applications where reduced precision is acceptable and memory efficiency is critical, such as in graphics processing, machine learning, and some scientific computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26a488e-ad4b-4ec4-a099-8434d5ad85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0.333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edaa4773-ada2-407f-9d08-c74142e3cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64-bit floating point\n",
    "tensor_fp64 = torch.tensor(value, dtype = torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0166b2a-de55-4366-af81-1ea6b2e2278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp64 tensor: 0.333000000000000018207657603852567262947559356689453125000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52fef4d-2924-4e98-a72a-e83cd98e4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_fp32 = torch.tensor(value, dtype = torch.float32)\n",
    "tensor_fp16 = torch.tensor(value, dtype = torch.float16)\n",
    "tensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9564f2d7-077f-4f6a-b2c7-3c0bfd75b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp64 tensor: 0.333000000000000018207657603852567262947559356689453125000000\n",
      "fp32 tensor: 0.333000004291534423828125000000000000000000000000000000000000\n",
      "fp16 tensor: 0.333007812500000000000000000000000000000000000000000000000000\n",
      "bf16 tensor: 0.332031250000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\n",
    "print(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\n",
    "print(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\n",
    "print(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb32c8b-8d9c-45e9-b8be-7f392e35d070",
   "metadata": {},
   "source": [
    "#### Downcasting\n",
    "\n",
    "- **Definition**: \n",
    "  - Downcasting refers to converting a variable from a larger data type to a smaller one, such as converting a 64-bit floating-point number (FP64) to a 32-bit floating-point number (FP32) or converting a 32-bit integer to a 16-bit integer.\n",
    "  - This process reduces the amount of memory used by the variable but may result in the loss of precision or overflow if the smaller type cannot accurately represent the original value.\n",
    "\n",
    "- **Examples of Downcasting**:\n",
    "  1. **Floating-Point Downcasting**:\n",
    "     - Converting a `float64` (double-precision) to `float32` (single-precision).\n",
    "     - Example:\n",
    "       ```python\n",
    "       import numpy as np\n",
    "       \n",
    "       # Original 64-bit floating-point\n",
    "       fp64_value = np.float64(123456789.123456789)\n",
    "       print(fp64_value)  # Output: 123456789.12345679\n",
    "       \n",
    "       # Downcast to 32-bit floating-point\n",
    "       fp32_value = np.float32(fp64_value)\n",
    "       print(fp32_value)  # Output: 123456792.0\n",
    "       ```\n",
    "       - **Observation**: The downcasting results in a loss of precision due to the smaller bit-width of `float32`.\n",
    "\n",
    "  2. **Integer Downcasting**:\n",
    "     - Converting an `int32` to `int16`.\n",
    "     - Example:\n",
    "       ```python\n",
    "       # Original 32-bit integer\n",
    "       int32_value = np.int32(32768)\n",
    "       print(int32_value)  # Output: 32768\n",
    "       \n",
    "       # Downcast to 16-bit integer\n",
    "       int16_value = np.int16(int32_value)\n",
    "       print(int16_value)  # Output: -32768 (due to overflow)\n",
    "       ```\n",
    "       - **Observation**: The downcasting results in an overflow, as `int16` can only represent values from -32768 to 32767.\n",
    "\n",
    "- **Potential Issues with Downcasting**:\n",
    "  - **Loss of Precision**: Especially in floating-point downcasting, where significant digits may be lost.\n",
    "  - **Overflow**: In integer downcasting, values that exceed the target type's range can wrap around or cause unexpected results.\n",
    "  - **Truncation**: When downcasting from a floating-point to an integer type, the fractional part is truncated, which can lead to inaccuracies.\n",
    "\n",
    "- **Usage Considerations**:\n",
    "  - Downcasting is useful in memory-constrained environments or when working with large datasets where precision is less critical.\n",
    "  - It's essential to ensure that downcasting does not lead to unacceptable errors in your computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386ee7b-b2a2-497f-8843-4a796eb9bb07",
   "metadata": {},
   "source": [
    "#### Use case\n",
    "- Mixed Precision training\n",
    "    - Do computation in smaller precision (FP16, BF16,FP8)\n",
    "    - Store and update the weights in higher precision (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fad5f-2245-4726-a77d-e053385c8aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
