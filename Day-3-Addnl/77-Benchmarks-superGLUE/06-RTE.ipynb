{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78887e62-e657-4bf8-96bf-43bd3d767d5c",
   "metadata": {},
   "source": [
    "#### CommitmentBank (CB): Overview\n",
    "The **CommitmentBank (CB)** is a benchmark dataset designed to evaluate a model's ability to perform **logical entailment** and **reasoning** over pairs of sentences. It focuses on understanding how well a model can discern the logical relationships between two sentences. Specifically, it tests a model's ability to determine whether the second sentence (the hypothesis) logically follows or contradicts the first sentence (the premise).\n",
    "\n",
    "#### Key Points About CommitmentBank (CB):\n",
    "- **Logical Entailment**: The model is asked to determine if the hypothesis logically follows from the premise.  \n",
    "  Example:  \n",
    "  - **Premise**: \"John is walking in the park.\"  \n",
    "  - **Hypothesis**: \"John is outdoors.\"  \n",
    "  - The model should recognize that the hypothesis logically follows from the premise (**entailment**).\n",
    "\n",
    "- **Contradiction**: The model is also asked to identify when the hypothesis contradicts the premise, meaning the hypothesis cannot be true if the premise is true.  \n",
    "  Example:  \n",
    "  - **Premise**: \"It is raining outside.\"  \n",
    "  - **Hypothesis**: \"The ground is dry.\"  \n",
    "  - The model should recognize that this is a contradiction, as rain implies wet ground.\n",
    "\n",
    "- **Neutrality**: The model should determine when the hypothesis neither follows nor contradicts the premise, i.e., it is independent or neutral.  \n",
    "  Example:  \n",
    "  - **Premise**: \"The sun is shining.\"  \n",
    "  - **Hypothesis**: \"The grass is green.\"  \n",
    "  - This could be neutral, as the premise doesn't directly support or contradict the hypothesis.\n",
    "\n",
    "#### Purpose of the Dataset:\n",
    "- **Assessing Reasoning Abilities**: The main aim is to assess a model's reasoning capabilities, particularly in understanding **entailment** and **contradiction** relationships.\n",
    "- **Evaluating Language Understanding**: It tests how well the model can interpret and make judgments about relationships between two sentences, which is a key component of **Natural Language Inference (NLI)** tasks.\n",
    "\n",
    "#### Usage:\n",
    "- **Model Evaluation**: CB can be used to benchmark a model's logical reasoning skills, especially in tasks where **sentence pair relationships** must be determined (e.g., in **NLI** tasks).\n",
    "- **Fine-tuning**: You can fine-tune pre-trained models on datasets like CommitmentBank to improve their performance on entailment and reasoning tasks.\n",
    "\n",
    "#### Summary:\n",
    "**CommitmentBank (CB)** provides a comprehensive framework to evaluate a model's ability to perform logical reasoning, a critical task for many NLP applications involving sentence understanding, inference, and reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0705964e-ee04-4474-8b90-2fdb9481c55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a16e38a-da0b-41ce-ac81-797d3918c1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6b893936ac4ce09691a31d232488aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254e8625c2504ae6b896e0f88052197d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b10f4f760448a986bacbcc67d73d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca36aa1527744e35804c823eb3658a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CommitmentBank (CB) dataset\n",
    "dataset = load_dataset(\"super_glue\", \"cb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c994684-38a5-4a29-b218-7f0cdca6de3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access train, validation, and test splits\n",
    "train_data      = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data       = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc38fff-3f73-46f5-be87-53d30f37e8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
       "        num_rows: 56\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8be8f-d371-4118-9bab-7156db46ca15",
   "metadata": {},
   "source": [
    "- The CommitmentBank (CB) dataset is indeed small, with approximately:\n",
    "\n",
    "    - Train: ~250 examples\n",
    "    - Validation: ~56 examples\n",
    "    - Test: ~250 examples\n",
    "    \n",
    "- This small size is intentional, as the dataset's primary goal is to evaluate a model's ability to perform nuanced logical entailment and reasoning tasks. \n",
    "\n",
    "- The focus is not on training a model from scratch but on assessing its generalization capabilities.\n",
    "\n",
    "- The CB dataset is particularly useful for benchmarking pre-trained models fine-tuned on similar tasks, like those in the SuperGLUE benchmark, and testing their ability to transfer knowledge to smaller, more challenging datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08547af4-9515-448b-b8b0-3f0f46b504be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set display options for Pandas\n",
    "pd.set_option('display.max_colwidth', None)  # No truncation of column content\n",
    "pd.set_option('display.width', None)         # No truncation of DataFrame display width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9669ef-fc2b-43ad-84b1-289ed9076200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the train, validation, and test splits to Pandas DataFrames\n",
    "train_df      = pd.DataFrame(train_data)\n",
    "validation_df = pd.DataFrame(validation_data)\n",
    "test_df       = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd7eddf-b750-443b-9f3c-c1a18cc2ef8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>This was a sheer waste of time. He would probably land and then tell them to walk back. When she glanced at him again he looked very grim and she wondered if she should have told Mitch that he might well have a lot of opportunity to photograph Spain - on foot as he walked back to Malaga.</td>\n",
       "      <td>Mitch might well have a lot of opportunity to photograph Spain</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>A: So, we're comparable. B: Yeah. A: As a matter of fact, I just paid my Richardson taxes because I live in Richardson and supplemented the Robin Hoods very thoroughly, I think. B: Yeah, I think Yeah, we have got it on the line, don't we.</td>\n",
       "      <td>they have got it on the line</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>How do you know? she was going to ask, but his smile was answer enough. If DeVore said there was going to be a vacancy there would be a vacancy.</td>\n",
       "      <td>there was going to be a vacancy</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage.</td>\n",
       "      <td>the American people are being shortchanged by the news coverage</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Miss Martindale had had a school, but her rigid ideas and stern manner had frightened the children, and their parents had taken them away. And gradually the school declined, until she had to give it up and retire to end her days in the white cottage with the inevitable cat as her only companion. Breeze had never imagined that digging was such hard work.</td>\n",
       "      <td>digging was such hard work</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more.</td>\n",
       "      <td>a lot of parents are that involved</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>B: No, it was, I didn't like the way it ended. A: I know, well the only reason I know why it ended is on Arsenio Hall one night, Christopher Reeves told, that, you know, B: Uh-huh. A: I can't believe they killed them.</td>\n",
       "      <td>they killed them</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly.</td>\n",
       "      <td>Morton is capable of interpreting this food for thought correctly</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant,</td>\n",
       "      <td>they do the packaging at this plant</td>\n",
       "      <td>188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time.</td>\n",
       "      <td>Jim Kelly is about to be swayed away from the Bills any time</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                          premise  \\\n",
       "83                                                                                                               This was a sheer waste of time. He would probably land and then tell them to walk back. When she glanced at him again he looked very grim and she wondered if she should have told Mitch that he might well have a lot of opportunity to photograph Spain - on foot as he walked back to Malaga.   \n",
       "220                                                                                                                                                                A: So, we're comparable. B: Yeah. A: As a matter of fact, I just paid my Richardson taxes because I live in Richardson and supplemented the Robin Hoods very thoroughly, I think. B: Yeah, I think Yeah, we have got it on the line, don't we.   \n",
       "78                                                                                                                                                                                                                                                               How do you know? she was going to ask, but his smile was answer enough. If DeVore said there was going to be a vacancy there would be a vacancy.   \n",
       "175                                                                                                                              A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage.   \n",
       "35                                            Miss Martindale had had a school, but her rigid ideas and stern manner had frightened the children, and their parents had taken them away. And gradually the school declined, until she had to give it up and retire to end her days in the white cottage with the inevitable cat as her only companion. Breeze had never imagined that digging was such hard work.   \n",
       "234  B: I do not know. I wonder where he gets it? You know, you must, I think TV is bad. Because they, uh, show all sorts of violence on, A: That and I do not think a lot of parents, I mean, I do not know how it is in the Air Force base. But, uh, I just do not think a lot of people, because of the economy, both need to work, you know. I just do not think a lot of parents are that involved any more.   \n",
       "231                                                                                                                                                                                     B: No, it was, I didn't like the way it ended. A: I know, well the only reason I know why it ended is on Arsenio Hall one night, Christopher Reeves told, that, you know, B: Uh-huh. A: I can't believe they killed them.   \n",
       "60                                                                                                                                I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly.   \n",
       "188                                                                                                                                                                                                                                                               B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant,   \n",
       "134                                                                                                                                                                                                                                          B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time.   \n",
       "\n",
       "                                                            hypothesis  idx  \\\n",
       "83      Mitch might well have a lot of opportunity to photograph Spain   83   \n",
       "220                                       they have got it on the line  220   \n",
       "78                                     there was going to be a vacancy   78   \n",
       "175    the American people are being shortchanged by the news coverage  175   \n",
       "35                                          digging was such hard work   35   \n",
       "234                                 a lot of parents are that involved  234   \n",
       "231                                                   they killed them  231   \n",
       "60   Morton is capable of interpreting this food for thought correctly   60   \n",
       "188                                they do the packaging at this plant  188   \n",
       "134       Jim Kelly is about to be swayed away from the Bills any time  134   \n",
       "\n",
       "     label  \n",
       "83       0  \n",
       "220      0  \n",
       "78       0  \n",
       "175      1  \n",
       "35       0  \n",
       "234      1  \n",
       "231      0  \n",
       "60       1  \n",
       "188      1  \n",
       "134      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368abc55-c596-4516-a69e-80278c5104ef",
   "metadata": {},
   "source": [
    "#### Output Columns in the DataFrames\n",
    "\n",
    "The **CommitmentBank (CB)** dataset includes the following fields:\n",
    "\n",
    "- **premise**: The main statement or context.\n",
    "- **hypothesis**: The statement to validate against the premise.\n",
    "- **label**: \n",
    "  - `0` for Entailment  \n",
    "  - `1` for Neutral  \n",
    "  - `2` for Contradiction  \n",
    "- **idx**: A unique identifier for each example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d493c4d-a086-408b-9fd9-d7ad70be7e7a",
   "metadata": {},
   "source": [
    "#### Pretrained Model\n",
    "\n",
    "- \"textattack/bert-base-uncased-SuperGLUE-CB\"\n",
    "- This is a fine-tuned BERT model trained on the CB dataset as part of the SuperGLUE benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45ec382f-172a-4966-b370-9beb6df71306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c5c9fb-380f-4f52-b41b-16b6b4edf074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_token ='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "394bd9cb-d0b0-4d09-a985-72e6ba19ecf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c405d50bd7d743c18ce2cd22d93652e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddbc41c7a1c4e79a2e0cd03091f0405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/489 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc9163aeb354494ac1738d454b312b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a32b63cfce435abb0aecf543acad70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751bd4b7264b42f9b2a5a4c9818d9ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"textattack/distilbert-base-uncased-RTE\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(model_name, token= access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f35fdbcb-c719-4a8b-abc2-9ce514ccde00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dummy data (Premise and Hypothesis)\n",
    "premise    = \"The sky is blue.\"\n",
    "hypothesis = \"The sky is not green.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "852ca70e-489a-4df4-bc4e-bd6ddc69b42f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the inputs\n",
    "inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd3dfe72-fad9-40f1-8f44-a9b995bc3b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform the inference (model's forward pass)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf5ffdce-3c12-4e17-8693-e91f5ffbaf13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the logits (raw scores)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da89baf-78be-4d71-870b-a83e752ba7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert logits to probabilities (softmax)\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09a0279a-8f8c-41fa-8294-c1914ef1bfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: The sky is blue.\n",
      "Hypothesis: The sky is not green.\n",
      "Predicted Class: 0 (0: Entailment, 1: Neutral, 2: Contradiction)\n",
      "Prediction Score: 0.7878133058547974\n"
     ]
    }
   ],
   "source": [
    "# Get predicted class (0: Entailment, 1: Neutral, 2: Contradiction)\n",
    "pred_class = logits.argmax().item()\n",
    "\n",
    "# Get prediction score (confidence)\n",
    "score = probabilities.max().item()\n",
    "\n",
    "# Print results\n",
    "print(f\"Premise: {premise}\")\n",
    "print(f\"Hypothesis: {hypothesis}\")\n",
    "print(f\"Predicted Class: {pred_class} (0: Entailment, 1: Neutral, 2: Contradiction)\")\n",
    "print(f\"Prediction Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94421d50-75c9-4414-9141-a26464520f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
