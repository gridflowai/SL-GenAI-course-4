{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2427080f-ea36-43cf-b8da-0c83c20adb20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Set display options for Pandas\n",
    "pd.set_option('display.max_colwidth', None)  # No truncation of column content\n",
    "pd.set_option('display.width', None)  # No truncation of DataFrame display width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d22f092-6c21-467c-b740-c55bb7db2d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5532964cd2554a7e8b316f0c4b851985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9500379b98b4fb59affab55f82f4877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/3.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c6e4b355b645569b3a7bce318f9861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a06ca49ba4044c688124dd0ee4c8733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564b7161d846422b83e4ac39e50c6a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b483c39998e04e9da2c0d324bb0e10c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the QQP dataset\n",
    "qqp_dataset = load_dataset(\"glue\", \"qqp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b015735-aefa-4273-a250-2c39b523e48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the train dataset to a Pandas DataFrame for analysis\n",
    "train_df = pd.DataFrame(qqp_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a9a6fa-964c-4945-a519-3cb920606da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the train dataset to a Pandas DataFrame for analysis\n",
    "valid_df = pd.DataFrame(qqp_dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4459ca3-72f8-4112-998f-c1c26e3d181c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((363846, 4), (40430, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8916c579-6fd8-40ff-9b58-82f593209867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "      <th>question1_length</th>\n",
       "      <th>question2_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311084</th>\n",
       "      <td>Is Magical Girl anime genre underrated in America?</td>\n",
       "      <td>Why are most Magical Girl anime underrated in America?</td>\n",
       "      <td>1</td>\n",
       "      <td>311084</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129991</th>\n",
       "      <td>What were the major effects of the cambodia earthquake, and how do these effects compare to the Cascadia earthquake in 1700?</td>\n",
       "      <td>What were the major effects of the cambodia earthquake, and how do these effects compare to the Arica earthquake in 1868?</td>\n",
       "      <td>1</td>\n",
       "      <td>129991</td>\n",
       "      <td>124</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327758</th>\n",
       "      <td>How difficult is aiims question paper?</td>\n",
       "      <td>How difficult is it to get selected at AIIMS New Delhi?</td>\n",
       "      <td>1</td>\n",
       "      <td>327758</td>\n",
       "      <td>38</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211711</th>\n",
       "      <td>Is really possible to earn 10k-15k from home based job?</td>\n",
       "      <td>How can I get a home based job and earn????</td>\n",
       "      <td>0</td>\n",
       "      <td>211711</td>\n",
       "      <td>55</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226455</th>\n",
       "      <td>Which movies are the best examples of the Hero's Journey?</td>\n",
       "      <td>What great films do not follow the classic '3-Act Structure' or 'Hero's Journey'?</td>\n",
       "      <td>0</td>\n",
       "      <td>226455</td>\n",
       "      <td>57</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           question1  \\\n",
       "311084                                                                            Is Magical Girl anime genre underrated in America?   \n",
       "129991  What were the major effects of the cambodia earthquake, and how do these effects compare to the Cascadia earthquake in 1700?   \n",
       "327758                                                                                        How difficult is aiims question paper?   \n",
       "211711                                                                       Is really possible to earn 10k-15k from home based job?   \n",
       "226455                                                                     Which movies are the best examples of the Hero's Journey?   \n",
       "\n",
       "                                                                                                                        question2  \\\n",
       "311084                                                                     Why are most Magical Girl anime underrated in America?   \n",
       "129991  What were the major effects of the cambodia earthquake, and how do these effects compare to the Arica earthquake in 1868?   \n",
       "327758                                                                    How difficult is it to get selected at AIIMS New Delhi?   \n",
       "211711                                                                                How can I get a home based job and earn????   \n",
       "226455                                          What great films do not follow the classic '3-Act Structure' or 'Hero's Journey'?   \n",
       "\n",
       "        label     idx  question1_length  question2_length  \n",
       "311084      1  311084                50                54  \n",
       "129991      1  129991               124               121  \n",
       "327758      1  327758                38                55  \n",
       "211711      0  211711                55                43  \n",
       "226455      0  226455                57                81  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258299c9-bf5b-4968-bf89-58f014ab3a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in training set: 363846\n"
     ]
    }
   ],
   "source": [
    "# Total samples\n",
    "total_samples = len(train_df)\n",
    "print(f\"Total samples in training set: {total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08623e1d-eae1-4782-8a88-a581b9238a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution:\n",
      "0    229468\n",
      "1    134378\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Duplicate and non-duplicate labels\n",
    "label_counts = train_df['label'].value_counts()\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8921f0b-9b56-4d1c-9dbc-1cc9993084cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Percentage Distribution:\n",
      "0    63.067342\n",
      "1    36.932658\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentage distribution of labels\n",
    "label_percentages = label_counts / total_samples * 100\n",
    "print(\"\\nLabel Percentage Distribution:\")\n",
    "print(label_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62affb3-3633-4889-945b-a3f4dc332ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Length of Question 1: 59.63\n",
      "Average Length of Question 2: 60.19\n"
     ]
    }
   ],
   "source": [
    "# Average length of questions\n",
    "train_df['question1_length'] = train_df['question1'].apply(lambda x: len(str(x)))\n",
    "train_df['question2_length'] = train_df['question2'].apply(lambda x: len(str(x)))\n",
    "\n",
    "avg_length_q1 = train_df['question1_length'].mean()\n",
    "avg_length_q2 = train_df['question2_length'].mean()\n",
    "print(f\"\\nAverage Length of Question 1: {avg_length_q1:.2f}\")\n",
    "print(f\"Average Length of Question 2: {avg_length_q2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a608cafe-8545-4843-b3cd-0aa60d236508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Question 1 and Question 2 lengths: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Correlation between question lengths\n",
    "correlation_length = train_df['question1_length'].corr(train_df['question2_length'])\n",
    "print(f\"Correlation between Question 1 and Question 2 lengths: {correlation_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d842407-9f94-4f55-b2d3-2b16a35f35da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6db20cdb-beeb-4182-bdde-e3da6a7bb06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the QQP validation dataset\n",
    "qqp_dataset_val = load_dataset(\"glue\", \"qqp\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2ce11ec-915e-43ac-8cc0-68581a1c38c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918b234ac819417190c0be0c11986047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be267d281ac54c68b7270c15b114c6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75949941727a4e17934e89e0eef8d27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d2c8c802a747d99e1a84215db6281b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16d5ae9a09c485ca2ee1273e68c2bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained model fine-tuned on QQP\n",
    "model_name  = \"textattack/bert-base-uncased-QQP\"\n",
    "classifier = pipeline(\"text-classification\", model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cffdedd4-bd36-4696-968f-fbd19a86ced0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'LABEL_1', 'score': 0.9737574458122253}\n"
     ]
    }
   ],
   "source": [
    "result = classifier({\n",
    "    \"text\": \"How difficult is AIIMS question paper?\",\n",
    "    \"text_pair\": \"How difficult is it to get selected at AIIMS New Delhi?\"\n",
    "})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff64894-5891-47de-916a-c7f9d9dabf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to test validation samples\n",
    "def test_validation_samples(dataset, classifier, num_samples=10):\n",
    "    print(\"Testing first\", num_samples, \"samples from validation set...\")\n",
    "    for i in range(num_samples):\n",
    "        sentence1 = dataset[i][\"question1\"]\n",
    "        sentence2 = dataset[i][\"question2\"]\n",
    "        label     = dataset[i][\"label\"]  # Ground truth label\n",
    "        \n",
    "        prediction = classifier({\"text_a\": sentence1, \"text_b\": sentence2})[0]\n",
    "        \n",
    "        predicted_label = prediction[\"label\"]\n",
    "        confidence      = prediction[\"score\"]\n",
    "        \n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Question 1: {sentence1}\")\n",
    "        print(f\"  Question 2: {sentence2}\")\n",
    "        print(f\"  Ground Truth: {label}\")\n",
    "        print(f\"  Predicted: {predicted_label} (Confidence: {confidence:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe61d044-c82a-4aa3-b9a4-9028401154fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing first 10 samples from validation set...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You need to specify either `text` or `text_target`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test first 10 validation samples\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_validation_samples(qqp_dataset_val, classifier, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mtest_validation_samples\u001b[1;34m(dataset, classifier, num_samples)\u001b[0m\n\u001b[0;32m      6\u001b[0m sentence2 \u001b[38;5;241m=\u001b[39m dataset[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m label     \u001b[38;5;241m=\u001b[39m dataset[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Ground truth label\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m prediction \u001b[38;5;241m=\u001b[39m classifier({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_a\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_b\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence2})[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m confidence      \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[1;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1299\u001b[0m         )\n\u001b[0;32m   1300\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\base.py:1308\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\text_classification.py:171\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[1;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# It used to be valid to use a list of list of list for text pairs, keeping this path for BC\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[0;32m    175\u001b[0m         text\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], text_pair\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[0;32m    176\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3015\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3013\u001b[0m all_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   3014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3015\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either `text` or `text_target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3017\u001b[0m     \u001b[38;5;66;03m# The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the\u001b[39;00m\n\u001b[0;32m   3018\u001b[0m     \u001b[38;5;66;03m# input mode in this case.\u001b[39;00m\n\u001b[0;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n",
      "\u001b[1;31mValueError\u001b[0m: You need to specify either `text` or `text_target`."
     ]
    }
   ],
   "source": [
    "# Test first 10 validation samples\n",
    "test_validation_samples(qqp_dataset_val, classifier, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77f092-6d58-4472-a848-6953517c216f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e2f40-5fa2-4e49-b2e7-55b13df8e82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
